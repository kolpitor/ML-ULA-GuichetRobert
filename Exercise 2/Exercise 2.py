# -*- coding: utf-8 -*-
"""Exo2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eeGFr9CZoPArz2oHPEcu8GsSRfoNHIh9

We import modules
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn import decomposition
from sklearn import preprocessing
from sklearn import discriminant_analysis

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.patheffects as PathEffects

"""We load datas"""

labels = np.load("labels.npy")
data = np.load("data.npy")

"""We create a panda's Dataframe with the datas"""

data = pd.DataFrame(
    data,
    index = labels,
    columns = ["s0", "s1", "s2", "s3", "s4", "s5"]
)
data.head()

"""We plot the range, their variance ans their mean of the datas for each sensors
<br><br><br>
We can see that the s4 have a larger range than the other

"""

data.boxplot()
plt.show()

"""We display the scatter matrix of the datas to see the precision of the sensors
<br><br><br>
S0 and S5 seem to be less accurate than the other but they're still relatively precise

S1 does not seem to work at all, its values are all centered around 0 with a small deviation

S2, S3, S4 seem to be the moste accurate and precise sensor
"""

pd.plotting.scatter_matrix(
    data,
    diagonal='kde', # Plot density functions
    figsize = (10, 10), 
)
plt.show()

"""We display the variance covariance matrix of the datas
<br><br><br>
We can see that the s4 have the biggest covariance with itself and the biggest variance with the others when the s1 have the lowest
"""

data.cov()

"""We display the correlation matrix of the datas
<br><br><br>
As expected we find high correlation values between S0, S2, S3, S4 and S5 but not with S1.

We can assume that he is broken
"""

data.corr()

"""Calculating Main components' eigenvalues.


Scikit-Learn PCA is not reduced by default. 
In addition, the divisor in the variance formula is that of an unbiased estimate, i.e. (n-1) instead of n.
<br><br><br>
We can see that only the first 3 dimensions are importants, the others are useless
"""

pca = decomposition.PCA()

eigenvalue = pca.fit(data).explained_variance_
eigenvalue

"""The eigenvectors are also the coefficients of the linear combinations of the variables allowing to define the principal variables."""

pca.components_.T
components = pca.fit(data).transform(data)
components

"""We look at the variance and standar deviation of the dimensions
<br><br><br>
Here again, we can see that only the first 3 dimensions are importants, the others are useless
"""

plt.boxplot(components)
plt.show()

"""We display the datas in the correct dimension, here in 2 dimensions
<br><br><br>
We can kind of see two classes but there is still an overlap at the center between the different labels so we cannot separate correctly our values
"""

plt.figure(figsize = (10,10))
for i, j, label in zip(components[:,0], components[:,1], data.index):
    plt.scatter(i, j, color = 'g') if label == 0 else plt.scatter(i, j, color = 'r')
    
plt.show()

"""The sign of an eigenvector is not determined because it is a direction or 
subspace that is "proper" for a matrix. Depending on the algorithm or software used, the vector can be oriented in one direction or the other but it is the same direction that is defined. This has no influence on the interpretation of these results.

The most the line go to the right, the most it impact the first dimension and the most the line go to the top, the most it impact the second dimension
<br><br><br>
We can see that the first dimension is in majority made with s4 and a bit with s2 and s3 and the second dimension is in majority made with s1 and s5 and just a little bit with s0
"""

c1 = pca.components_[0] 
c2 = pca.components_[1] 

fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(1, 1, 1)

for i, j, label in zip(c1, c2, data.columns):
    plt.text(i, j, label, fontsize=16)
    plt.arrow(0, 0, i, j, color='black')

plt.axis((-1.2, 1.2, -1.2, 1.2))
circle = plt.Circle((0,0), radius=1, color='gray', fill=False)
ax.add_patch(circle)
plt.show()

"""We display the datas in the correct dimension, here in 3 dimensions
<br><br><br>
In 3D we can find a plane that separete perfectly the classes, allowing us to predict the label based on the projected component with minimum error
"""

fig = plt.figure(figsize = (10,10))
ax = fig.add_subplot(projection='3d')
for i, j, k, label in zip(components[:,0], components[:,1], components[:,2], data.index):
    ax.scatter(i, j, k, color = 'g') if label == 0 else ax.scatter(i, j, k, color = 'r')
    
ax.view_init(120, 5)
plt.show()

"""Like the 2 dimensions, we display the importance a each sensor for each 3 dimensions. It's complicate to show it on an 2 dimensional plot so we display their representation on each dimension so on 3 plots.

- first plot :
  - right : first dimension
  - top : second dimension
- second plot :
  - right : first dimension
  - top : third dimension
- third plot :
  - right : second dimension
  - top : third dimension

<br><br><br>
We can see that the first dimension is made in majority of s4 with a bit of s2 and s3 but negate a bit by s5, the second is made in majority with s1 and s5 and the third dimension is in majority made with s3, s0 and s5 but is negate by s1 and a bit of s2
"""

c1 = pca.components_[0]
c2 = pca.components_[1]
c3 = pca.components_[2]

for dim1, dim2 in [[c1,c2], [c1,c3], [c2, c3]]:
    fig = plt.figure(figsize = (10,10))
    ax = fig.add_subplot(1, 1, 1)
    for i, j, label in zip(dim1, dim2, data.columns):
        ax.text(i, j, label, fontsize=16)
        ax.arrow(0, 0, i, j, color='black')
        
    plt.axis((-1.2, 1.2, -1.2, 1.2))
    circle = plt.Circle((0,0), radius=1, color='gray', fill=False)
    ax.add_patch(circle)
    plt.show()